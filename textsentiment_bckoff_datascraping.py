# -*- coding: utf-8 -*-
"""TextSentiment_BckOff_DataScraping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10DDxIcVa_NuLz0RhEsnbg2wcKFfjHSGg
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import requests
from bs4 import BeautifulSoup

page = pd.read_excel('/content/drive/MyDrive/BlackOffer/20211030 Test Assignment/Input.xlsx')

ID = []
currPage = []
pageUrl = []
for index, row in page.iterrows():
  ID.append(row['URL_ID'])
  pageUrl.append(row['URL'])
  currPage.append(requests.get(row['URL']).text)

"""#Fetching text content of each page using class of div in which text is present."""

page_text = []
for i in currPage:
    soup = BeautifulSoup(i, 'lxml')
    # Find all div tags
    div_tags = soup.find_all('div')
    found = False  # Flag to track if the desired class is found in any div tag
    # Iterate over div tags to find the one containing the desired content
    for div_tag in div_tags:
        # Check if the div tag contains the desired class
        if 'td-post-content' in div_tag.get('class', []):
            # Extract text from the div tag and strip any leading or trailing whitespace
            page_text.append(div_tag.text.strip())
            found = True
    if not found:
        # If 'td-post-content' class is not found in any div tag, append 'NULL'
        page_text.append('NULL')

# print(soup.prettify())

"""len(page_text) here, helps us to know that we access content of all the available pages."""

len(page_text)

page_text

df = pd.DataFrame({'id':ID, 'Url':pageUrl, 'text':page_text})

ids_with_null_text = df.loc[df['text'] == 'NULL', 'id']

# Print the IDs where the text is 'NULL'
print("IDs where the text is 'NULL':")
print(ids_with_null_text)

df

df.drop(index=35, inplace=True)

df.drop(index=48, inplace=True)

df.shape

"""#Creating list of stop words."""

# Initialize an empty list to store stop words
stop_words = []

# List of stop words file names
stop_words_files = ['/content/drive/MyDrive/BlackOffer/20211030 Test Assignment/StopWords/StopWords_Auditor.txt', '/content/drive/MyDrive/BlackOffer/20211030 Test Assignment/StopWords/StopWords_Currencies.txt', '/content/drive/MyDrive/BlackOffer/20211030 Test Assignment/StopWords/StopWords_DatesandNumbers.txt', '/content/drive/MyDrive/BlackOffer/20211030 Test Assignment/StopWords/StopWords_Generic.txt', '/content/drive/MyDrive/BlackOffer/20211030 Test Assignment/StopWords/StopWords_GenericLong.txt', '/content/drive/MyDrive/BlackOffer/20211030 Test Assignment/StopWords/StopWords_Geographic.txt', '/content/drive/MyDrive/BlackOffer/20211030 Test Assignment/StopWords/StopWords_Names.txt']  # Add your file names here

# Read stop words from each file
for stop_words_file in stop_words_files:
    # Open the stop words text file
    with open(stop_words_file, 'r', encoding='latin-1') as stop_file:
        # Read each line in the file
        for line in stop_file:
            # Remove any leading/trailing whitespace and newline characters
            stop_word = line.strip()
            # Add the stop word to the list if it's not already in the list
            if stop_word not in stop_words:
                stop_words.append(stop_word)

stop_words
len(stop_words)

"""#Creating positive words dictionary excluding stop words"""

# Initialize an empty list to store positive score words
positive_score_words = []

# Open the positive score text file
with open('/content/drive/MyDrive/BlackOffer/20211030 Test Assignment/MasterDictionary/positive-words.txt', 'r') as file:
    # Read each line in the file
    for line in file:
        # Remove any leading/trailing whitespace and newline characters
        word = line.strip()
        # Add the word to the list if it's not in the stop words list
        if word not in stop_words:
            positive_score_words.append(word)

# Create a dictionary with a single key and the list of words as its value
positive_words_dict = {'pos_score': positive_score_words}

len(positive_score_words)

"""#Creating negative words dictionary excluding stop words"""

# Initialize an empty list to store positive score words
negative_score_words = []

# Open the positive score text file
with open('/content/drive/MyDrive/BlackOffer/20211030 Test Assignment/MasterDictionary/negative-words.txt', 'r', encoding='latin-1') as file:
    # Read each line in the file
    for line in file:
        # Remove any leading/trailing whitespace and newline characters
        word = line.strip()
        # Add the word to the list if it's not in the stop words list
        if word not in stop_words:
            negative_score_words.append(word)

# Create a dictionary with a single key and the list of words as its value
negative_words_dict = {'neg_score': negative_score_words}

len(negative_score_words)

print(positive_words_dict)

print(negative_words_dict)

import nltk
nltk.download('punkt')
from nltk.tokenize import  word_tokenize

# Function to remove stop words from text
def remove_stopwords(text):
    tokens = word_tokenize(text)
    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]
    return ' '.join(filtered_tokens)

# Apply remove_stopwords function to each row in 'text_column'
df['text'] = df['text'].apply(remove_stopwords)

df

negative_words_dict

"""## Function to calculate positive score, negative scores, polarity score and subjectivity score for each row."""

def calculate_sentiment_scores(text):
    positive_score = 0
    negative_score = 0
    total_words_after_cleaning = 0
    words = text.split() # Split text into words
    for word in words:
        if word in positive_words_dict['pos_score']:
            positive_score += 1
        elif word in negative_words_dict['neg_score']:
            negative_score -= 1
        total_words_after_cleaning += 1

    negative_score = negative_score*(-1)
    polarity_score = (positive_score-negative_score)/ ((positive_score + negative_score) + 0.000001)
    subjectivity_score = (positive_score + negative_score)/ ((total_words_after_cleaning) + 0.000001)

    return positive_score, negative_score, polarity_score, subjectivity_score


# Apply the function to each row of the DataFrame
df[['Positive Score', 'Negative Score', 'Polarity Score', 'Subjectivity Score']] = df['text'].apply(lambda x: pd.Series(calculate_sentiment_scores(x)))

df

df['Polarity Score'].describe()

df.isnull().sum()

"""# Average Sentence Length = the number of words / the number of sentences
# Percentage of Complex words = the number of complex words / the number of words
# Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)
# Average Number of Words Per Sentence = the total number of words / the total number of sentences
"""

#pip install --upgrade syllapy

#pip install syllables

#pip list

pip install textstat

import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
import string
from textstat import syllable_count

nltk.download('punkt')
nltk.download('stopwords')

def count_syllables(word):
    # Count syllables in each word
    # Exclude syllables for words ending with "es" or "ed"
    if word.endswith("es") or word.endswith("ed"):
        return syllable_count(word) - 1
    else:
        return syllable_count(word)

def calculate_metrics(text):
    # Tokenize the text into sentences
    sentences = sent_tokenize(text)

    # Tokenize the text into words
    words = word_tokenize(text)

    # Remove punctuation
    words = [word.strip(string.punctuation) for word in words if word.strip(string.punctuation)]

    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word.lower() not in stop_words]

    # Count syllables and calculate syllable count per word
    syllable_counts_per_word = {word: count_syllables(word) for word in words}

    # Count syllables in each word
    syllable_counts = list(syllable_counts_per_word.values())

    # Calculate average sentence length
    average_sentence_length = len(words) / len(sentences)

    # Calculate percentage of complex words (words with more than two syllables)
    complex_word_count = sum(1 for count in syllable_counts if count > 2)
    percentage_complex_words = (complex_word_count / len(words)) * 100

    # Calculate Fog Index
    fog_index = 0.4 * (average_sentence_length + percentage_complex_words)

    # Calculate average number of words per sentence
    average_words_per_sentence = len(words) / len(sentences)

    # Calculate word count
    word_count = len(words)

    # Calculate total syllables
    total_syllables = sum(syllable_counts)

    # Calculate average number of syllables per word
    average_syllables_per_word = total_syllables / word_count

    return {
        'average_sentence_length': average_sentence_length,
        'percentage_complex_words': percentage_complex_words,
        'fog_index': fog_index,
        'average_words_per_sentence': average_words_per_sentence,
        'complex_word_count': complex_word_count,
        'word_count': word_count,
        'syllable_counts_per_word': average_syllables_per_word,
    }

df[['Average Sentence Length', 'Percentage of Complex Words', 'Fog Index', 'Average Number of Words Per Sentence','Complex Word Count','word_count','Syllable Count Per Wword']]  = df['text'].apply(lambda x: pd.Series(calculate_metrics(x)))

import re

def count_personal_pronouns(text):
    # Define personal pronouns
    personal_pronouns = ['I', 'we', 'my', 'ours', 'us']

    # Compile regex pattern to match personal pronouns as whole words
    pattern = re.compile(r'\b(?:' + '|'.join(personal_pronouns) + r')\b', re.IGNORECASE)

    # Find matches in the text
    matches = re.findall(pattern, text)

    # Count the total number of matches
    count = len(matches)

    return count

df[['personal_pronoun']]  = df['text'].apply(lambda x: pd.Series(count_personal_pronouns(x)))

def calculate_average_word_length(text):
    # Tokenize the text into words
    words = nltk.word_tokenize(text)

    # Calculate total number of characters in all words
    total_characters = sum(len(word) for word in words)

    # Calculate total number of words
    total_words = len(words)

    # Calculate average word length
    if total_words > 0:
        average_word_length = total_characters / total_words
    else:
        average_word_length = 0

    return average_word_length

df[['average_word_length']]  = df['text'].apply(lambda x: pd.Series(calculate_average_word_length(x)))

df

df = df.drop('text', axis=1)

df

column_names = {'id': 'URL_ID',
                'Url':'URL',
                'Positive Score':'POSITIVE SCORE',
                'Negative Score':'NEGATIVE SCORE',
                'Polarity Score':'POLARITY SCORE',
                'Subjectivity Score':'SUBJECTIVITY SCORE',
                'Average Sentence Length':'AVG SENTENCE LENGTH',
                'Percentage of Complex Words':'PERCENTAGE OF COMPLEX WORDS',
                'Fog Index':'FOG INDEX',
                'Average Number of Words Per Sentence':'AVG NUMBER OF WORDS PER SENTENCE',
                'Complex Word Count':'COMPLEX WORD COUNT',
                'word_count':'WORD COUNT',
                'Syllable Count Per Wword':'SYLLABLE PER WORD',
                'personal_pronoun':'PERSONAL PRONOUNS',
                'average_word_length':'AVG WORD LENGTH'}

df = df.rename(columns = column_names)

df

import pandas as pd
from openpyxl import Workbook

def write_to_excel(data, filename):
    # Create a new Workbook
    wb = Workbook()

    # Create a new worksheet
    ws = wb.active

    # Write data to the worksheet
    for row_data in data:
        ws.append(row_data)

    # Save the workbook to a file
    wb.save(filename)

filename = 'Output.xlsx'

# Convert DataFrame to list of lists
data = [df.columns.tolist()] + df.values.tolist()

# Call the function to write data to Excel
write_to_excel(data, filename)